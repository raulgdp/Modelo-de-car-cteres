{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Character-Based Neural Language Model in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A language model predicts the next word in the sequence based on the specific words that have come before it in the sequence. This comes at the cost of requiring larger models that are slower to train. Nevertheless, in the field of neural language models, character-based models offer a lot of promise for a general, flexible and powerful approach to language modeling. In this section you will know:\n",
    " How to prepare text for character-based language modeling.\n",
    " How to develop a character-based language model using LSTMs.\n",
    " How to use a trained character-based language model to generate text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "\n",
    "The first step is to prepare the text data. We will start by defining the type of language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text\n",
    "# define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text \n",
    "Next, we need to clean the loaded text. We will not do much to it on this example. Specifically,\n",
    "we will strip all of the new line characters so that we have one long sequence of characters\n",
    "separated only by white space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save tokens to file, one dialog per line\n",
    "def save_doc(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    "\n",
    "# load text\n",
    "raw_text = load_doc('el_quijote.txt')\n",
    "#raw_text = load_doc('/floyd/input/dataset2/el_quijote.txt')\n",
    "#raw_text = load_doc('rhyme.txt')\n",
    "\n",
    "#print(raw_text)\n",
    "# clean\n",
    "tokens = raw_text.split()\n",
    "raw_text = ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Sequences\n",
    "Now that we have a long list of characters, we can create our input-output sequences used to\n",
    "train the model. Each input sequence will be 10 characters with one output character, making\n",
    "each sequence 11 characters long. We can create the sequences by enumerating the characters\n",
    "in the text, starting at the 11th character at index 10. The sequences are save in a file with\n",
    "function save_doc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 1038375\n"
     ]
    }
   ],
   "source": [
    "# organize into sequences of characters\n",
    "length = 20\n",
    "sequences = list()\n",
    "for i in range(length, len(raw_text)):\n",
    "\t# select sequence of tokens\n",
    "\tseq = raw_text[i-length:i+1]\n",
    "\t# store\n",
    "\tsequences.append(seq)\n",
    "print('Total Sequences: %d' % len(sequences))\n",
    "# save sequences to file\n",
    "out_filename = 'char_sequences.txt'\n",
    "#print (sequences)\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Language Model\n",
    "\n",
    "The model will read encoded characters and predict the next character in the sequence. The first step is to load the prepared character sequence data from char sequences.txt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "  # open the file as read only\n",
    "  file = open(filename, 'r' )\n",
    "  # read all text\n",
    "  text = file.read()\n",
    "  # close the file\n",
    "  file.close()\n",
    "  return text\n",
    "# load\n",
    "in_filename = 'char_sequences.txt'\n",
    "raw_text = load_doc(in_filename)\n",
    "lines = raw_text.split( '\\n' )\n",
    "#print lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Mapping\n",
    "We can create the mapping given a sorted set of unique characters in the\n",
    "raw input data. The mapping is a dictionary of character values to integer values.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 89\n",
      "{'t': 70, '<': 22, 'q': 67, 'B': 25, '”': 88, '8': 18, 'F': 29, '9': 19, ':': 20, 'a': 52, 'U': 44, 'd': 55, 'r': 68, 'V': 45, ']': 51, '[': 50, 'g': 58, 'I': 32, '¿': 79, '4': 14, 'E': 28, \"'\": 4, 'A': 24, 'M': 36, '1': 11, '-': 8, '«': 77, 'l': 62, '0': 10, 'v': 72, ';': 21, 'N': 37, 'x': 73, 'C': 26, '\\n': 0, '’': 86, '6': 16, '‘': 85, 'D': 27, '(': 5, '́': 81, '2': 12, 'H': 31, 'e': 56, 'p': 66, '3': 13, '?': 23, '!': 2, ' ': 1, '̈': 83, 'W': 46, 'u': 71, 'Y': 48, 'T': 43, 'R': 41, '¡': 76, 'K': 34, 'b': 53, 'P': 39, '\"': 3, '–': 84, '̀': 80, 'm': 63, 'n': 64, 's': 69, '̃': 82, 'G': 30, 'Q': 40, '.': 9, 'O': 38, 'S': 42, '“': 87, ',': 7, 'Z': 49, '5': 15, 'j': 61, '»': 78, 'L': 35, 'h': 59, 'X': 47, 'y': 74, 'i': 60, ')': 6, 'o': 65, 'J': 33, 'z': 75, 'c': 54, '7': 17, 'f': 57}\n"
     ]
    }
   ],
   "source": [
    "# integer encode sequences of characters\n",
    "chars = sorted(list(set(raw_text)))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "sequences = list()\n",
    "for line in lines:\n",
    "\t# integer encode line\n",
    "\tencoded_seq = [mapping[char] for char in line]\n",
    "\t# store\n",
    "\tsequences.append(encoded_seq)\n",
    "# vocabulary size\n",
    "vocab_size = len(mapping)\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode Sequences\n",
    "The sequences of characters must be encoded as integers. This means that each unique character\n",
    "will be assigned a specific integer value and each sequence of characters will be encoded as a\n",
    "sequence of integers. We can separate the columns into input and\n",
    "output sequences of characters. We can do this using a simple array slice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectorización de secuencias\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] [[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] este es el [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# separate into input and output\n",
    "sequences = array(sequences)\n",
    "print('vectorización de secuencias')\n",
    "#print (sequences[:,:-1],sequences[:,-1])\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "#print ('estas son las secuenciassssss', X,'de yyyyyy',y)\n",
    "sequences = [to_categorical(x, num_classes=vocab_size) for x in X]\n",
    "X = array(sequences)\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "print (X[0],X[1],'este es el',y[0],y[1])\n",
    "# define model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the language model\n",
    "The model is defined with an input layer that takes sequences that have 10 time steps and 38\n",
    "features for the one hot encoded input sequences. The model has a single LSTM hidden layer with 75 memory cells, chosen with a little trial and error. The model has a fully connected output layer that outputs one vector with a probability distribution across all characters in the vocabulary. A softmax activation function is used on\n",
    "the output layer to ensure the output has the properties of a probability distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 75)                34200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 38)                2888      \n",
      "=================================================================\n",
      "Total params: 37,088\n",
      "Trainable params: 37,088\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      " - 1s - loss: 3.6175 - acc: 0.0852\n",
      "Epoch 2/100\n",
      " - 0s - loss: 3.5401 - acc: 0.1880\n",
      "Epoch 3/100\n",
      " - 0s - loss: 3.2740 - acc: 0.1905\n",
      "Epoch 4/100\n",
      " - 0s - loss: 3.0777 - acc: 0.1905\n",
      "Epoch 5/100\n",
      " - 0s - loss: 3.0246 - acc: 0.1905\n",
      "Epoch 6/100\n",
      " - 0s - loss: 2.9931 - acc: 0.1905\n",
      "Epoch 7/100\n",
      " - 0s - loss: 2.9760 - acc: 0.1905\n",
      "Epoch 8/100\n",
      " - 0s - loss: 2.9636 - acc: 0.1905\n",
      "Epoch 9/100\n",
      " - 0s - loss: 2.9500 - acc: 0.1905\n",
      "Epoch 10/100\n",
      " - 0s - loss: 2.9286 - acc: 0.1905\n",
      "Epoch 11/100\n",
      " - 0s - loss: 2.9114 - acc: 0.1905\n",
      "Epoch 12/100\n",
      " - 0s - loss: 2.8894 - acc: 0.1905\n",
      "Epoch 13/100\n",
      " - 0s - loss: 2.8708 - acc: 0.1905\n",
      "Epoch 14/100\n",
      " - 0s - loss: 2.8441 - acc: 0.1905\n",
      "Epoch 15/100\n",
      " - 0s - loss: 2.8191 - acc: 0.2105\n",
      "Epoch 16/100\n",
      " - 0s - loss: 2.7926 - acc: 0.2356\n",
      "Epoch 17/100\n",
      " - 0s - loss: 2.7547 - acc: 0.2105\n",
      "Epoch 18/100\n",
      " - 0s - loss: 2.7218 - acc: 0.2531\n",
      "Epoch 19/100\n",
      " - 0s - loss: 2.6879 - acc: 0.2256\n",
      "Epoch 20/100\n",
      " - 0s - loss: 2.6439 - acc: 0.2707\n",
      "Epoch 21/100\n",
      " - 0s - loss: 2.6103 - acc: 0.2581\n",
      "Epoch 22/100\n",
      " - 0s - loss: 2.5635 - acc: 0.3008\n",
      "Epoch 23/100\n",
      " - 0s - loss: 2.5163 - acc: 0.3058\n",
      "Epoch 24/100\n",
      " - 0s - loss: 2.4696 - acc: 0.3158\n",
      "Epoch 25/100\n",
      " - 0s - loss: 2.4225 - acc: 0.3258\n",
      "Epoch 26/100\n",
      " - 0s - loss: 2.3728 - acc: 0.3233\n",
      "Epoch 27/100\n",
      " - 0s - loss: 2.3248 - acc: 0.3333\n",
      "Epoch 28/100\n",
      " - 0s - loss: 2.2792 - acc: 0.3509\n",
      "Epoch 29/100\n",
      " - 0s - loss: 2.2232 - acc: 0.3759\n",
      "Epoch 30/100\n",
      " - 0s - loss: 2.1756 - acc: 0.3835\n",
      "Epoch 31/100\n",
      " - 0s - loss: 2.1194 - acc: 0.3960\n",
      "Epoch 32/100\n",
      " - 0s - loss: 2.0762 - acc: 0.4160\n",
      "Epoch 33/100\n",
      " - 0s - loss: 2.0409 - acc: 0.4185\n",
      "Epoch 34/100\n",
      " - 0s - loss: 2.0046 - acc: 0.4536\n",
      "Epoch 35/100\n",
      " - 0s - loss: 1.9402 - acc: 0.4436\n",
      "Epoch 36/100\n",
      " - 0s - loss: 1.8937 - acc: 0.4812\n",
      "Epoch 37/100\n",
      " - 0s - loss: 1.8479 - acc: 0.4561\n",
      "Epoch 38/100\n",
      " - 0s - loss: 1.7943 - acc: 0.5038\n",
      "Epoch 39/100\n",
      " - 0s - loss: 1.7437 - acc: 0.5038\n",
      "Epoch 40/100\n",
      " - 0s - loss: 1.7132 - acc: 0.5138\n",
      "Epoch 41/100\n",
      " - 0s - loss: 1.6563 - acc: 0.5489\n",
      "Epoch 42/100\n",
      " - 0s - loss: 1.6141 - acc: 0.5363\n",
      "Epoch 43/100\n",
      " - 0s - loss: 1.5702 - acc: 0.5664\n",
      "Epoch 44/100\n",
      " - 0s - loss: 1.5373 - acc: 0.5589\n",
      "Epoch 45/100\n",
      " - 0s - loss: 1.5007 - acc: 0.6241\n",
      "Epoch 46/100\n",
      " - 0s - loss: 1.4553 - acc: 0.5940\n",
      "Epoch 47/100\n",
      " - 0s - loss: 1.4080 - acc: 0.6040\n",
      "Epoch 48/100\n",
      " - 0s - loss: 1.3778 - acc: 0.6341\n",
      "Epoch 49/100\n",
      " - 0s - loss: 1.3495 - acc: 0.6466\n",
      "Epoch 50/100\n",
      " - 0s - loss: 1.3154 - acc: 0.6541\n",
      "Epoch 51/100\n",
      " - 0s - loss: 1.2781 - acc: 0.6767\n",
      "Epoch 52/100\n",
      " - 0s - loss: 1.2384 - acc: 0.6842\n",
      "Epoch 53/100\n",
      " - 0s - loss: 1.2022 - acc: 0.7143\n",
      "Epoch 54/100\n",
      " - 0s - loss: 1.1851 - acc: 0.6867\n",
      "Epoch 55/100\n",
      " - 0s - loss: 1.1480 - acc: 0.7243\n",
      "Epoch 56/100\n",
      " - 0s - loss: 1.1137 - acc: 0.7343\n",
      "Epoch 57/100\n",
      " - 0s - loss: 1.0790 - acc: 0.7469\n",
      "Epoch 58/100\n",
      " - 0s - loss: 1.0442 - acc: 0.7845\n",
      "Epoch 59/100\n",
      " - 0s - loss: 1.0040 - acc: 0.7920\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.9722 - acc: 0.8070\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.9468 - acc: 0.8145\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.9265 - acc: 0.8120\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.8948 - acc: 0.8371\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.8423 - acc: 0.8471\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.8226 - acc: 0.8647\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.8076 - acc: 0.8722\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.7700 - acc: 0.8596\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.7466 - acc: 0.8997\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.7094 - acc: 0.8897\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.6929 - acc: 0.9098\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.6622 - acc: 0.9173\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.6405 - acc: 0.9198\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.6202 - acc: 0.9323\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.5997 - acc: 0.9298\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.5673 - acc: 0.9524\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.5572 - acc: 0.9424\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.5286 - acc: 0.9474\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.5226 - acc: 0.9424\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.4990 - acc: 0.9499\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.4841 - acc: 0.9524\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.4660 - acc: 0.9574\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.4457 - acc: 0.9574\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.4218 - acc: 0.9674\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.4108 - acc: 0.9674\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.3934 - acc: 0.9724\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.3871 - acc: 0.9699\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.3592 - acc: 0.9799\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.3463 - acc: 0.9774\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.3322 - acc: 0.9825\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.3198 - acc: 0.9875\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.3109 - acc: 0.9850\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.2955 - acc: 0.9900\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.2861 - acc: 0.9875\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.2736 - acc: 0.9900\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.2615 - acc: 0.9925\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.2503 - acc: 0.9925\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.2436 - acc: 0.9950\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.2367 - acc: 0.9950\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.2303 - acc: 0.9950\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.2407 - acc: 0.9900\n"
     ]
    }
   ],
   "source": [
    "from pickle import dump\n",
    "def define_model(X):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(75, input_shape=(X.shape[1], X.shape[2])))\n",
    "    model.add(Dense(vocab_size, activation= 'softmax' ))\n",
    "    # compile model\n",
    "    model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])\n",
    "    # summarize defined model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file= 'model.png' , show_shapes=True)\n",
    "    return model\n",
    "model = define_model(X)\n",
    "# fit model\n",
    "model.fit(X, y, epochs=100, verbose=2)\n",
    "# save the model to file\n",
    "model.save( '/home/raul/clases-pln/modelo-caracter/model.h5' )\n",
    "# save the mapping\n",
    "dump(mapping, open( '/home/raul/clases-pln/modelo-caracter/mapping.pkl' , 'wb' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c': 17, 'k': 24, 'p': 29, 't': 33, \"'\": 2, 'S': 12, 'a': 15, 'w': 35, ',': 3, 'l': 25, ' ': 1, 'T': 13, 'B': 7, 'E': 9, 'H': 11, '\\n': 0, 'F': 10, 'o': 28, 'q': 30, 'C': 8, ';': 5, 'h': 22, 'y': 37, 'd': 18, 'x': 36, 'e': 19, 'W': 14, 'm': 26, 'r': 31, 'i': 23, 'f': 20, 'g': 21, '.': 4, 'n': 27, 'u': 34, 'A': 6, 's': 32, 'b': 16}\n",
      "Sing a song of sixpence, A pocket f\n",
      "king was in his counting house, Cou\n",
      "The queen was in the garden, Hangi\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pickle import load\n",
    "#from numpy import array\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# generate a sequence of characters with a language model\n",
    "def generate_seq(model, mapping, seq_length, seed_text, n_chars):\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of characters\n",
    "    for _ in range(n_chars):\n",
    "        # encode the characters as integers\n",
    "        encoded = [mapping[char] for char in in_text]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # one hot encode\n",
    "        encoded = to_categorical(encoded, num_classes=len(mapping))\n",
    "        #encoded = encoded.reshape(1, encoded.shape[0], encoded.shape[1])\n",
    "        # predict character\n",
    "        yhat = model.predict_classes(encoded, verbose=0)\n",
    "        #print(yhat)\n",
    "        # reverse map integer to character\n",
    "        out_char = ''\n",
    "        for char, index in mapping.items():\n",
    "            if index == yhat:\n",
    "                out_char = char\n",
    "                break \n",
    "        # append to input\n",
    "        in_text += out_char\n",
    "    return in_text\n",
    "# load the model\n",
    "model = load_model('model.h5')\n",
    "# load the mapping\n",
    "mapping = load(open('mapping.pkl', 'rb'))\n",
    "print(mapping)\n",
    "# test start of rhyme\n",
    "print(generate_seq(model, mapping, 10, 'Sing a son', 25))\n",
    "# test mid-line\n",
    "print(generate_seq(model, mapping, 10, 'king was i', 25))\n",
    "# test not in original\n",
    "print(generate_seq(model, mapping, 10, 'The queen', 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
